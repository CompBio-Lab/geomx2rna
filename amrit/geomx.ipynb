{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bef16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import io, transforms\n",
    "from torchvision.utils import Image, ImageDraw\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063264c",
   "metadata": {},
   "source": [
    "## import sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df67ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('data/disease1B_scan/disease1B_scan.png').convert('RGB')\n",
    "transform = transforms.Compose([\n",
    " transforms.ToTensor()\n",
    "])\n",
    "img = transform(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b83c8",
   "metadata": {},
   "source": [
    "#### Channels:\t\n",
    " - FITC/525 nm : SYTO 13 : DNA (Grey)\n",
    " - Cy3/568 nm : Alexa 532 : PanCK (Yellow)\n",
    " - Texas Red/615 nm : Alexa 594 : CD45 (Cyan)\n",
    " - Cy5/666 nm : Cy5 : Custom (Magenta)\n",
    "\n",
    "**SYTO** Deep Red Nucleic Acid Stain is cell-permeant dye that specifically stains the nuclei of live, dead, or fixed cells.\n",
    "\n",
    "**pan-CK** (AE1/AE3) and EMA are epithelium-specific antibodies. As the basic component of cellular structure of normal epithelial cells and epithelial cancer cells, they are often used to differentiate tumors according to whether they originate from the epithelium or not.\n",
    "\n",
    "**CD45** is a signalling molecule that is an essential regulator of T and B cell antigen receptor signalling.\n",
    "\n",
    "**CD10+CD31** â€“ Proximal nephrons and endothelial cells (Custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc726e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_pil_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926782aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('data/normal2B_scan/normal2B_scan.png').convert('RGB')\n",
    "transform = transforms.Compose([\n",
    " transforms.ToTensor()\n",
    "])\n",
    "img = transform(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa840e",
   "metadata": {},
   "source": [
    "## image resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4636e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 3\n",
    "IMG_SIZE = 256*f\n",
    "PATCH_SIZE = 256\n",
    "\n",
    "resize = transforms.Resize((IMG_SIZE, IMG_SIZE))\n",
    "resized_img = resize(img)\n",
    "resized_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf6f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_image(resized_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc8babc",
   "metadata": {},
   "source": [
    "## Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = resized_img.unfold(1, PATCH_SIZE, PATCH_SIZE).unfold(2, PATCH_SIZE, PATCH_SIZE)\n",
    "\n",
    "dataset = []\n",
    "\n",
    "fig, ax = plt.subplots(f, f, figsize=(16, 16))\n",
    "for i in range(f):\n",
    "    for j in range(f):\n",
    "        sub_img = patches[:, i, j]\n",
    "        dataset.append(sub_img.unsqueeze(0))\n",
    "        ax[i][j].imshow(to_pil_image(sub_img))\n",
    "        ax[i][j].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ad39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = dataset[0]\n",
    "\n",
    "output = model(input)\n",
    "output = F.softmax(output, dim=1)\n",
    "prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "\n",
    "pred_label_idx.squeeze_()\n",
    "predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n",
    "print('Predicted:', predicted_label, '(', prediction_score.squeeze().item(), ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026bba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IntegratedGradients object and get attributes\n",
    "integrated_gradients = IntegratedGradients(model)\n",
    "attributions_ig = integrated_gradients.attribute(input, target=pred_label_idx, n_steps=200)\n",
    "\n",
    "# create custom colormap for visualizing the result\n",
    "default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                 [(0, '#ffffff'),\n",
    "                                                  (0.25, '#000000'),\n",
    "                                                  (1, '#000000')], N=256)\n",
    "\n",
    "\n",
    "# visualize the results using the visualize_image_attr helper method\n",
    "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                             np.transpose(input.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                             methods=[\"original_image\", \"heat_map\"],\n",
    "                             signs=['all', 'positive'],\n",
    "                             cmap=default_cmap,\n",
    "                             show_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91592b18",
   "metadata": {},
   "source": [
    "## train ResNet with trian and validation set of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d6536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(dict_images, n_patches = 9, ref_group = 'normal'):\n",
    "    PATCH_SIZE = 256\n",
    "    f = int(np.sqrt(n_patches))\n",
    "    IMG_SIZE = PATCH_SIZE * f\n",
    "    resize = transforms.Resize((IMG_SIZE, IMG_SIZE))\n",
    "    transform = transforms.Compose([\n",
    "     transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    images = {}\n",
    "    for group in dict_images.keys():\n",
    "        dataset = []\n",
    "        for img in dict_images[group]:\n",
    "            ## import image\n",
    "            img = Image.open('data/'+img+'/'+img+'.png').convert('RGB')\n",
    "            ## convert img to tensor\n",
    "            img = transform(img)\n",
    "            ## resize image\n",
    "            resized_img = resize(img)\n",
    "            ## create patches\n",
    "            patches = resized_img.unfold(1, PATCH_SIZE, PATCH_SIZE).unfold(2, PATCH_SIZE, PATCH_SIZE)\n",
    "            ## reshape data\n",
    "            for i in range(f):\n",
    "                for j in range(f):\n",
    "                    sub_img = patches[:, i, j]\n",
    "                    if group == ref_group:\n",
    "                        data_target = (sub_img, 0)\n",
    "                    else:\n",
    "                        data_target = (sub_img, 1)\n",
    "                    dataset.append(data_target)\n",
    "            images[group] = dataset\n",
    "    return images\n",
    "\n",
    "class Custom_Dataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, _dataset):\n",
    "        self.dataset = _dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example, target = self.dataset[index]\n",
    "        return example, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {'dkd': ['disease1B_scan', 'disease2B_scan'],\n",
    "         'normal': ['normal2B_scan', 'normal4_scan']}\n",
    "valid = {'dkd': ['disease3_scan', 'disease4_scan'],\n",
    "         'normal': ['normal3_scan']}\n",
    "test = {'dkd': ['disease4_scan'],\n",
    "        'normal': ['normal4_scan']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16b8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train loader\n",
    "train_datasets = create_datasets(dict_images = train, n_patches = 100, ref_group = 'normal')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=Custom_Dataset(train_datasets['dkd'] + train_datasets['normal']),\n",
    "                                   batch_size=1,\n",
    "                                   shuffle=False)\n",
    "\n",
    "## validation loader\n",
    "valid_datasets = create_datasets(dict_images = valid, n_patches = 100, ref_group = 'normal')\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=Custom_Dataset(valid_datasets['dkd'] + valid_datasets['normal']),\n",
    "                                   batch_size=1,\n",
    "                                   shuffle=False)\n",
    "\n",
    "## test loader\n",
    "test_datasets = create_datasets(dict_images = test, n_patches = 100, ref_group = 'normal')\n",
    "test_loader = torch.utils.data.DataLoader(dataset=Custom_Dataset(test_datasets['dkd'] + test_datasets['normal']),\n",
    "                                   batch_size=1,\n",
    "                                   shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1581fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(train_datasets['dkd'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f623688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# Hyperparameters\n",
    "in_channel = 3\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(2048, 2)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.resnet18(pretrained=True)\n",
    "net = net.to(device)\n",
    "net\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    _,pred = torch.max(out, dim=1)\n",
    "    return torch.sum(pred==labels).item()\n",
    "\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs, 2)\n",
    "net.fc = net.fc.to(device)\n",
    "\n",
    "n_epochs = 5\n",
    "print_every = 10\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    for batch_idx, (data_, target_) in enumerate(train_loader):\n",
    "        data_, target_ = data_.to(device), target_.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(data_)\n",
    "        loss = criterion(outputs, target_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==target_).item()\n",
    "        total += target_.size(0)\n",
    "        if (batch_idx) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
    "    batch_loss = 0\n",
    "    total_t=0\n",
    "    correct_t=0\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for data_t, target_t in (valid_loader):\n",
    "            data_t, target_t = data_t.to(device), target_t.to(device)\n",
    "            outputs_t = net(data_t)\n",
    "            loss_t = criterion(outputs_t, target_t)\n",
    "            batch_loss += loss_t.item()\n",
    "            _,pred_t = torch.max(outputs_t, dim=1)\n",
    "            correct_t += torch.sum(pred_t==target_t).item()\n",
    "            total_t += target_t.size(0)\n",
    "        val_acc.append(100 * correct_t/total_t)\n",
    "        val_loss.append(batch_loss/len(valid_loader))\n",
    "        network_learned = batch_loss < valid_loss_min\n",
    "        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')\n",
    "\n",
    "        \n",
    "        if network_learned:\n",
    "            valid_loss_min = batch_loss\n",
    "            torch.save(net.state_dict(), 'resnet.pt')\n",
    "            print('Improvement-Detected, save-model')\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548918a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.title(\"Train-Validation Accuracy\")\n",
    "plt.plot(train_acc, label='train')\n",
    "plt.plot(val_acc, label='validation')\n",
    "plt.xlabel('num_epochs', fontsize=12)\n",
    "plt.ylabel('accuracy', fontsize=12)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    z = model(x)\n",
    "    _, yhat = torch.max(z.data, 1)\n",
    "    print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de2979",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=30\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "N_test=len(valid_datasets['dkd']) + len(valid_datasets['normal'])\n",
    "COST=0\n",
    "\n",
    "def train_model(n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        COST=0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x)\n",
    "            loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            COST+=loss.data\n",
    "        \n",
    "        cost_list.append(COST)\n",
    "        correct=0\n",
    "        #perform a prediction on the validation  data  \n",
    "        for x_test, y_test in valid_loader:\n",
    "            z = model(x_test)\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            print(_)\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "        print(correct)\n",
    "        print(N_test)\n",
    "        accuracy = correct / N_test\n",
    "        accuracy_list.append(accuracy)\n",
    "     \n",
    "train_model(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8007c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a1711",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(cost_list, color=color)\n",
    "ax1.set_xlabel('epoch', color=color)\n",
    "ax1.set_ylabel('Cost', color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color) \n",
    "ax2.set_xlabel('epoch', color=color)\n",
    "ax2.plot( accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', color=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4fad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50aa63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Custom_Dataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, _dataset):\n",
    "        self.dataset = _dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example, target = self.dataset[index]\n",
    "        return example, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# train_data = [([1, 3, 5], 0),\n",
    "#               ([2, 4, 6], 1)]\n",
    "train_loader = torch.utils.data.DataLoader(dataset=Custom_Dataset(train_data),\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=False)\n",
    "\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.squeeze(0).shape)\n",
    "    print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db97a697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb4419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = resize(Image.open('data/disease1B_scan/disease1B_scan.png').convert('RGB'))\n",
    "transform = transforms.Compose([\n",
    " transforms.ToTensor()\n",
    "])\n",
    "img = transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb8843",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = img.unfold(1, PATCH_SIZE, PATCH_SIZE).unfold(2, PATCH_SIZE, PATCH_SIZE)\n",
    "\n",
    "dataset = []\n",
    "\n",
    "fig, ax = plt.subplots(10, 10, figsize=(12, 12))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        sub_img = patches[:, i, j]\n",
    "        dataset.append(sub_img.unsqueeze(0))\n",
    "        ax[i][j].imshow(to_pil_image(sub_img))\n",
    "        ax[i][j].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf14c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches.shape, len(dataset), dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches.permute(4, 3, 0, 1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f229371",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = dataset[0].unsqueeze(0)\n",
    "\n",
    "output = model(input)\n",
    "output = F.softmax(output, dim=1)\n",
    "prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "\n",
    "pred_label_idx.squeeze_()\n",
    "predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n",
    "print('Predicted:', predicted_label, '(', prediction_score.squeeze().item(), ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a510d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_tensors[18].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7801b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles, mask_p, patches_base, dim = split_tensor(img.unsqueeze(0), tile_size=256)\n",
    "type(tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad5c0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = patches.reshape(3, -1, PATCH_SIZE, PATCH_SIZE)\n",
    "patches.transpose_(0, 1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 16, figsize=(12, 12))\n",
    "for i in range(16):\n",
    "    ax[i].imshow(to_pil_image(patches[i]))\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826319f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tensor(tensor, tile_size=256):\n",
    "    mask = torch.ones_like(tensor)\n",
    "    # use torch.nn.Unfold\n",
    "    stride  = tile_size//2\n",
    "    unfold  = torch.nn.Unfold(kernel_size=(tile_size, tile_size), stride=stride)\n",
    "    # Apply to mask and original image\n",
    "    mask_p  = unfold(mask)\n",
    "    patches = unfold(tensor)\n",
    "\t\n",
    "    patches = patches.reshape(3, tile_size, tile_size, -1).permute(3, 0, 1, 2)\n",
    "    if tensor.is_cuda:\n",
    "        patches_base = torch.zeros(patches.size(), device=tensor.get_device())\n",
    "    else: \n",
    "        patches_base = torch.zeros(patches.size())\n",
    "\t\n",
    "    tiles = []\n",
    "    for t in range(patches.size(0)):\n",
    "         tiles.append(patches[[t], :, :, :])\n",
    "    return tiles, mask_p, patches_base, (tensor.size(2), tensor.size(3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1580b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size=256\n",
    "tensor=img.unsqueeze(0)\n",
    "stride=256\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f595d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones_like(tensor)\n",
    "# use torch.nn.Unfold\n",
    "stride  = tile_size//2\n",
    "unfold  = torch.nn.Unfold(kernel_size=(tile_size, tile_size), stride=stride)\n",
    "# Apply to mask and original image\n",
    "mask_p  = unfold(mask)\n",
    "patches = unfold(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1127609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b421f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import GradientShap\n",
    "from captum.attr import Saliency\n",
    "from captum.attr import NoiseTunnel\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    " transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_normalize = transforms.Normalize(\n",
    "     mean=[0.485, 0.456, 0.406],\n",
    "     std=[0.229, 0.224, 0.225]\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdebeafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P / https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = 'imagenet_class_index.json'\n",
    "with open(labels_path) as json_data:\n",
    "    idx_to_labels = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e0827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e4ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open('data/disease1B_scan/disease1B_scan - 001.png').convert('RGB')\n",
    "\n",
    "transformed_img = transform(img)\n",
    "\n",
    "rgb_image = transform_normalize(transformed_img)\n",
    "#input = input.unsqueeze(0)\n",
    "rgb_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6560dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 224\n",
    "stride = 1\n",
    "patches = rgb_image.data.unfold(0, 3, 3).unfold(1, patch_size, stride).unfold(2, patch_size, stride)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e01b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(patches.shape)\n",
    "x = patches[:,torch.from_numpy(np.arange(0,a[1])),:,:,:,:].split(1, dim=1)\n",
    "for i in list(np.arange(a[1])):\n",
    "    \n",
    "    y =  x[i][:,:,torch.from_numpy(np.arange(0,a[2])),:,:,:].split(1, dim=2)\n",
    "    for j in list(np.arange(a[2])):\n",
    "        img = to_pil(y[j].squeeze(0).squeeze(0).squeeze(0))\n",
    "        img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d313ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 256\n",
    "def split_tensor(tensor, tile_size=256):\n",
    "    mask = torch.ones_like(tensor)\n",
    "    # use torch.nn.Unfold\n",
    "    stride  = tile_size//2\n",
    "    unfold  = torch.nn.Unfold(kernel_size=(tile_size, tile_size), stride=stride)\n",
    "    # Apply to mask and original image\n",
    "    mask_p  = unfold(mask)\n",
    "    patches = unfold(tensor)\n",
    "\t\n",
    "    patches = patches.reshape(3, tile_size, tile_size, -1).permute(3, 0, 1, 2)\n",
    "    if tensor.is_cuda:\n",
    "        patches_base = torch.zeros(patches.size(), device=tensor.get_device())\n",
    "    else: \n",
    "        patches_base = torch.zeros(patches.size())\n",
    "\t\n",
    "    tiles = []\n",
    "    for t in range(patches.size(0)):\n",
    "         tiles.append(patches[[t], :, :, :])\n",
    "    return tiles, mask_p, patches_base, (tensor.size(2), tensor.size(3))\n",
    "\n",
    "def rebuild_tensor(tensor_list, mask_t, base_tensor, t_size, tile_size=256):\n",
    "    stride  = tile_size//2  \n",
    "    # base_tensor here is used as a container\n",
    "\n",
    "    for t, tile in enumerate(tensor_list):\n",
    "         print(tile.size())\n",
    "         base_tensor[[t], :, :] = tile  \n",
    "\t \n",
    "    base_tensor = base_tensor.permute(1, 2, 3, 0).reshape(3*tile_size*tile_size, base_tensor.size(0)).unsqueeze(0)\n",
    "    fold = torch.nn.Fold(output_size=(t_size[0], t_size[1]), kernel_size=(tile_size, tile_size), stride=stride)\n",
    "    # https://discuss.pytorch.org/t/seemlessly-blending-tensors-together/65235/2?u=bowenroom\n",
    "    output_tensor = fold(base_tensor)/fold(mask_t)\n",
    "    # output_tensor = fold(base_tensor)\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b0399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%time\n",
    "file = 'data/disease1B_scan/disease1B_scan - 001.png'\n",
    "image_size=2560\n",
    "Loader = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()])\n",
    "input_tensor = Loader(Image.open(file).convert('RGB')).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Split image into overlapping tiles\n",
    "tile_tensors, mask_t, base_tensor, t_size = split_tensor(input_tensor, 256)\n",
    "\n",
    "\n",
    "# Put tiles back together\n",
    "output_tensor = rebuild_tensor(tile_tensors, mask_t, base_tensor, t_size, 256)\n",
    "\n",
    "# Save Output\n",
    "Image2PIL = transforms.ToPILImage()\n",
    "print(f'the whole length of the patches is {len(tile_tensors)}')\n",
    "# show small patches\n",
    "for i in range(len(tile_tensors)):\n",
    "    print(f'the current is {i}')\n",
    "    Image2PIL(tile_tensors[i].cpu().squeeze(0))\n",
    "print('the reconstruct image')\n",
    "Image2PIL(output_tensor.cpu().squeeze(0))\n",
    "# Image2PIL(output_tensor.cpu().squeeze(0)).save('output_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor=input_tensor\n",
    "mask = torch.ones_like(tensor)\n",
    "# use torch.nn.Unfold\n",
    "stride  = tile_size//2\n",
    "unfold  = torch.nn.Unfold(kernel_size=(tile_size, tile_size), stride=stride)\n",
    "# Apply to mask and original image\n",
    "mask_p  = unfold(mask)\n",
    "patches = unfold(tensor)\n",
    "\n",
    "patches = patches.reshape(3, tile_size, tile_size, -1).permute(3, 0, 1, 2)\n",
    "if tensor.is_cuda:\n",
    "    patches_base = torch.zeros(patches.size(), device=tensor.get_device())\n",
    "else: \n",
    "    patches_base = torch.zeros(patches.size())\n",
    "\n",
    "tiles = []\n",
    "for t in range(patches.size(0)):\n",
    "     tiles.append(patches[[t], :, :, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf1453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdcbc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tile_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd4a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 10,figsize=(15, 15))\n",
    "\n",
    "for i in range(1):\n",
    "    for j in range(10):\n",
    "        axes[i,j].imshow(Image2PIL(tile_tensors[j].cpu().squeeze(0)))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ae1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, axarr = plt.subplots(2,2, figsize=(15, 15))\n",
    "axarr[0,0].imshow(Image2PIL(tile_tensors[0].cpu().squeeze(0)))\n",
    "axarr[0,1].imshow(Image2PIL(tile_tensors[1].cpu().squeeze(0)))\n",
    "axarr[1,0].imshow(Image2PIL(tile_tensors[2].cpu().squeeze(0)))\n",
    "axarr[1,1].imshow(Image2PIL(tile_tensors[3].cpu().squeeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4402f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f61fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_pred(x):\n",
    "    output = model(x)\n",
    "    output = F.softmax(output, dim=1)\n",
    "    prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "\n",
    "    pred_label_idx.squeeze_()\n",
    "    predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n",
    "    return predicted_label, prediction_score.squeeze().item()\n",
    "\n",
    "\n",
    "for ind, x in enumerate(tile_tensors):\n",
    "    print(ind, make_pred(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc3ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tile_tensors[18]\n",
    "\n",
    "output = model(input)\n",
    "output = F.softmax(output, dim=1)\n",
    "prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "\n",
    "pred_label_idx.squeeze_()\n",
    "predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n",
    "print('Predicted:', predicted_label, '(', prediction_score.squeeze().item(), ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f4ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tile_tensors[0]\n",
    "\n",
    "output = model(input)\n",
    "output = F.softmax(output, dim=1)\n",
    "prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "\n",
    "pred_label_idx.squeeze_()\n",
    "predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n",
    "print('Predicted:', predicted_label, '(', prediction_score.squeeze().item(), ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open('data/disease1B_scan/disease1B_scan - 001.png').convert('RGB')\n",
    "\n",
    "transformed_img = transform(img)\n",
    "\n",
    "input = transform_normalize(transformed_img)\n",
    "input = input.unsqueeze(0)\n",
    "\n",
    "output = model(input)\n",
    "output = F.softmax(output, dim=1)\n",
    "prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "\n",
    "pred_label_idx.squeeze_()\n",
    "predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n",
    "print('Predicted:', predicted_label, '(', prediction_score.squeeze().item(), ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2402d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IntegratedGradients object and get attributes\n",
    "integrated_gradients = IntegratedGradients(model)\n",
    "attributions_ig = integrated_gradients.attribute(input, target=pred_label_idx, n_steps=200)\n",
    "\n",
    "# create custom colormap for visualizing the result\n",
    "default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                 [(0, '#ffffff'),\n",
    "                                                  (0.25, '#000000'),\n",
    "                                                  (1, '#000000')], N=256)\n",
    "\n",
    "\n",
    "# visualize the results using the visualize_image_attr helper method\n",
    "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                             np.transpose(input.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                             methods=[\"original_image\", \"heat_map\"],\n",
    "                             signs=['all', 'positive'],\n",
    "                             cmap=default_cmap,\n",
    "                             show_colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bae87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tifffile import imread\n",
    "img = imread('sample_data/nuclei.tif')\n",
    "\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e99e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformed_img = transform(img)\n",
    "\n",
    "input = transform_normalize(transformed_img)\n",
    "input = input.unsqueeze(0)\n",
    "\n",
    "output = model(input)\n",
    "output = F.softmax(output, dim=1)\n",
    "prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "\n",
    "pred_label_idx.squeeze_()\n",
    "predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n",
    "print('Predicted:', predicted_label, '(', prediction_score.squeeze().item(), ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "import PIL\n",
    "import os\n",
    "import mimetypes\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "from skimage.io import imread\n",
    "from natsort import natsorted\n",
    "import re\n",
    "import numba\n",
    "from fastai2.vision.all import *\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToPILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f60df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.resnet50(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7bcb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=qaDe0qQZ5AQ&t=42s&ab_channel=AladdinPersson\n",
    "model2.fc = torch.nn.Linear(2048, 2)\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetCos(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.x=images\n",
    "        self.y=labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample = self.x[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        if (sample.float().size()[0] == 3):\n",
    "            return [sample.float(), self.y[idx]]\n",
    "\n",
    "images = tile_tensors\n",
    "gp1_labels = [0]*44\n",
    "gp2_labels = [1]*44\n",
    "labels = gp1_labels + gp2_labels\n",
    "dataset= DataSetCos(torch.tensor(images), torch.tensor(labels), transform =None)\n",
    "dataloader= torch.utils.data.DataLoader(dataset,batch_size=128,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c971b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bcde2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 20\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [44, 44])\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb949bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    print(batch_idx)\n",
    "    print(data.shape)\n",
    "    print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a01cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# Hyperparameters\n",
    "in_channel = 3\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model\n",
    "model = torchvision.models.googlenet(pretrained=True)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import KMNIST\n",
    "\n",
    "# define the train and val splits\n",
    "TRAIN_SPLIT = 0.75\n",
    "VAL_SPLIT = 1 - TRAIN_SPLIT\n",
    "\n",
    "# load the KMNIST dataset\n",
    "print(\"[INFO] loading the KMNIST dataset...\")\n",
    "trainData = KMNIST(root=\"data\", train=True, download=True,\n",
    "\ttransform=ToTensor())\n",
    "testData = KMNIST(root=\"data\", train=False, download=True,\n",
    "\ttransform=ToTensor())\n",
    "# calculate the train/validation split\n",
    "print(\"[INFO] generating the train/validation split...\")\n",
    "numTrainSamples = int(len(trainData) * TRAIN_SPLIT)\n",
    "numValSamples = int(len(trainData) * VAL_SPLIT)\n",
    "(trainData, valData) = random_split(trainData,\n",
    "\t[numTrainSamples, numValSamples],\n",
    "\tgenerator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6499cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecc060",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "#train and test data directory\n",
    "data_dir = \"images/\"\n",
    "test_data_dir = \"images/\"\n",
    "\n",
    "\n",
    "#load the train and test data\n",
    "dataset = ImageFolder(data_dir,transform = transforms.Compose([\n",
    "    transforms.Resize((150,150)),transforms.ToTensor()\n",
    "]))\n",
    "test_dataset = ImageFolder(test_data_dir,transforms.Compose([\n",
    "    transforms.Resize((150,150)),transforms.ToTensor()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = dataset[0]\n",
    "\n",
    "print(img.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaec2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.classes, len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(img,label):\n",
    "    print(f\"Label : {dataset.classes[label]}\")\n",
    "    plt.imshow(img.permute(1,2,0))\n",
    "\n",
    "#display the first image in the dataset\n",
    "display_img(*dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 20\n",
    "val_size = 50\n",
    "train_size = len(dataset) - val_size \n",
    "\n",
    "train_data,val_data = random_split(dataset,[train_size,val_size])\n",
    "print(f\"Length of Train Data : {len(train_data)}\")\n",
    "print(f\"Length of Validation Data : {len(val_data)}\")\n",
    "\n",
    "#output\n",
    "#Length of Train Data : 12034\n",
    "#Length of Validation Data : 2000\n",
    "\n",
    "#load the train and validation into batches.\n",
    "train_dl = DataLoader(train_data, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "val_dl = DataLoader(val_data, batch_size*2, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac958d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_batch(dl):\n",
    "    \"\"\"Plot images grid of single batch\"\"\"\n",
    "    for images, labels in dl:\n",
    "        fig,ax = plt.subplots(figsize = (8,12))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,nrow=8).permute(1,2,0))\n",
    "        break\n",
    "        \n",
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167398ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        \n",
    "\n",
    "model = ImageClassificationBase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://discuss.pytorch.org/t/pipe-pretrained-model-to-custom-layers/67836/4\n",
    "class MyResnet50(models.resnet.ResNet):\n",
    "    def __init__(self, pretrained=False):\n",
    "        # Pass default resnet50 arguments to super init\n",
    "        # https://github.com/pytorch/vision/blob/e130c6cca88160b6bf7fea9b8bc251601a1a75c5/torchvision/models/resnet.py#L260\n",
    "        super(MyResnet50, self).__init__(models.resnet.Bottleneck, [3, 4, 6, 3])\n",
    "        if pretrained:\n",
    "            self.load_state_dict(models.resnet50(pretrained=True).state_dict())\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "model = MyResnet50(pretrained=True)\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "output = model(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "  \n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e619d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaturalSceneClassification(ImageClassificationBase):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512,2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01679ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaturalSceneClassification()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88657ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001\n",
    "#fitting the model on training data and record the result after each epoch\n",
    "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c03df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# Hyperparameters\n",
    "in_channel = 3\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model\n",
    "model = torchvision.models.googlenet(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(2048, 2)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_dl):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed65da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651bb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6bc37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1c6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.fc.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad14a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, n_classes)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.fc.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tile_tensors[0].shape)\n",
    "model_ft(tile_tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f6902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tile_tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f199c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(i.squeeze(0), 0) for i in tile_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39405efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Custom_Dataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, _dataset):\n",
    "        self.dataset = _dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example, target = self.dataset[index]\n",
    "        return example, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# train_data = [([1, 3, 5], 0),\n",
    "#               ([2, 4, 6], 1)]\n",
    "train_loader = torch.utils.data.DataLoader(dataset=Custom_Dataset(train_data),\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=False)\n",
    "\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.squeeze(0).shape)\n",
    "    print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# Hyperparameters\n",
    "in_channel = 3\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model\n",
    "n_classes = 2\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, n_classes)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = model_ft(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer_ft.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer_ft.step()\n",
    "\n",
    "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba727e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f12b6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
